Project: /docs/reference/js/_project.yaml
Book: /docs/reference/_book.yaml
page_type: reference

{% comment %}
DO NOT EDIT THIS FILE!
This is generated by the JS SDK team, and any local changes will be
overwritten. Changes should be made in the source code at
https://github.com/firebase/firebase-js-sdk
{% endcomment %}

# UsageMetadata interface
Usage metadata about a [GenerateContentResponse](./ai.generatecontentresponse.md#generatecontentresponse_interface)<!-- -->.

<b>Signature:</b>

```typescript
export interface UsageMetadata 
```

## Properties

|  Property | Type | Description |
|  --- | --- | --- |
|  [cachedContentTokenCount](./ai.usagemetadata.md#usagemetadatacachedcontenttokencount) | number | The number of tokens in the prompt that were served from the cache. If implicit caching is not active or no content was cached, this will be 0. |
|  [cacheTokensDetails](./ai.usagemetadata.md#usagemetadatacachetokensdetails) | [ModalityTokenCount](./ai.modalitytokencount.md#modalitytokencount_interface)<!-- -->\[\] | Detailed breakdown of the cached tokens by modality (e.g., text, image). This list provides granular insight into which parts of the content were cached. |
|  [candidatesTokenCount](./ai.usagemetadata.md#usagemetadatacandidatestokencount) | number |  |
|  [candidatesTokensDetails](./ai.usagemetadata.md#usagemetadatacandidatestokensdetails) | [ModalityTokenCount](./ai.modalitytokencount.md#modalitytokencount_interface)<!-- -->\[\] |  |
|  [promptTokenCount](./ai.usagemetadata.md#usagemetadataprompttokencount) | number |  |
|  [promptTokensDetails](./ai.usagemetadata.md#usagemetadataprompttokensdetails) | [ModalityTokenCount](./ai.modalitytokencount.md#modalitytokencount_interface)<!-- -->\[\] |  |
|  [thoughtsTokenCount](./ai.usagemetadata.md#usagemetadatathoughtstokencount) | number | The number of tokens used by the model's internal "thinking" process. |
|  [toolUsePromptTokenCount](./ai.usagemetadata.md#usagemetadatatooluseprompttokencount) | number | The number of tokens used by tools. |
|  [toolUsePromptTokensDetails](./ai.usagemetadata.md#usagemetadatatooluseprompttokensdetails) | [ModalityTokenCount](./ai.modalitytokencount.md#modalitytokencount_interface)<!-- -->\[\] | A list of tokens used by tools, broken down by modality. |
|  [totalTokenCount](./ai.usagemetadata.md#usagemetadatatotaltokencount) | number |  |

## UsageMetadata.cachedContentTokenCount

The number of tokens in the prompt that were served from the cache. If implicit caching is not active or no content was cached, this will be 0.

<b>Signature:</b>

```typescript
cachedContentTokenCount?: number;
```

## UsageMetadata.cacheTokensDetails

Detailed breakdown of the cached tokens by modality (e.g., text, image). This list provides granular insight into which parts of the content were cached.

<b>Signature:</b>

```typescript
cacheTokensDetails?: ModalityTokenCount[];
```

## UsageMetadata.candidatesTokenCount

<b>Signature:</b>

```typescript
candidatesTokenCount: number;
```

## UsageMetadata.candidatesTokensDetails

<b>Signature:</b>

```typescript
candidatesTokensDetails?: ModalityTokenCount[];
```

## UsageMetadata.promptTokenCount

<b>Signature:</b>

```typescript
promptTokenCount: number;
```

## UsageMetadata.promptTokensDetails

<b>Signature:</b>

```typescript
promptTokensDetails?: ModalityTokenCount[];
```

## UsageMetadata.thoughtsTokenCount

The number of tokens used by the model's internal "thinking" process.

<b>Signature:</b>

```typescript
thoughtsTokenCount?: number;
```

## UsageMetadata.toolUsePromptTokenCount

The number of tokens used by tools.

<b>Signature:</b>

```typescript
toolUsePromptTokenCount?: number;
```

## UsageMetadata.toolUsePromptTokensDetails

A list of tokens used by tools, broken down by modality.

<b>Signature:</b>

```typescript
toolUsePromptTokensDetails?: ModalityTokenCount[];
```

## UsageMetadata.totalTokenCount

<b>Signature:</b>

```typescript
totalTokenCount: number;
```
